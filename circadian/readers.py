# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/api/05_readers.ipynb.

# %% auto 0
__all__ = ['VALID_WEREABLE_STREAMS', 'WereableAccessor', 'load_json', 'load_csv', 'load_actiwatch', 'resample_df',
           'distribute_stream_over_duration', 'combine_wereable_dataframes']

# %% ../nbs/api/05_readers.ipynb 4
import json
import numpy as np
import pandas as pd
from typing import Dict

# %% ../nbs/api/05_readers.ipynb 6
VALID_WEREABLE_STREAMS = ['steps', 'heartrate', 'wake', 'light_estimate', 'activity']

# %% ../nbs/api/05_readers.ipynb 7
@pd.api.extensions.register_dataframe_accessor("wereable")
class WereableAccessor:
    "pd.DataFrame accessor implementing wereable-specific methods"
    def __init__(self, pandas_obj):
        self._validate_columns(pandas_obj)
        self._obj = pandas_obj

    @staticmethod
    def _validate_columns(obj):
        if 'datetime' not in obj.columns:
            raise AttributeError("DataFrame must have 'datetime' column.")

        if not any([col in obj.columns for col in VALID_WEREABLE_STREAMS]):
            raise AttributeError(f"DataFrame must have at least one wereable data column from: {VALID_WEREABLE_STREAMS}.")
        
    @staticmethod
    def _validate_metadata(metadata):
        if not isinstance(metadata, dict):
            raise AttributeError("Metadata must be a dictionary.")
        if not any([key in metadata.keys() for key in ['data_id', 'subject_id']]):
            raise AttributeError("Metadata must have at least one of the following keys: data_id, subject_id.")
        if not all([isinstance(value, str) for value in metadata.values()]):
            raise AttributeError("Metadata values must be strings.")
    
    def is_valid(self):
        self._validate_columns(self._obj)
        self._validate_metadata(self._obj.attrs)
        return True

    def add_metadata(self,
                     metadata: Dict[str, str], # metadata containing data_id, subject_id, or other_info
                     inplace: bool = False, # whether to return a new dataframe or modify the current one
                     ):
        self._validate_metadata(metadata)
        if inplace:
            for key, value in metadata.items():
                self._obj.attrs[key] = value
        else:
            obj = self._obj.copy()
            for key, value in metadata.items():
                obj.attrs[key] = value
            return obj


    def plot(self, 
             name: str, # name of the wereable data to plot (one of steps, heartrate, wake, light_estimate, or activity)
             ax=None, # matplotlib axes
             *args, # arguments to pass to matplotlib.pyplot.plot
             **kwargs # keyword arguments to pass to matplotlib.pyplot.plot
             ):
        if name not in VALID_WEREABLE_STREAMS:
            raise AttributeError(f"Name must be one of: {VALID_WEREABLE_STREAMS}.")
        if ax is None:
            ax = self._obj.plot(x='datetime', y=name, *args, **kwargs)
        else:
            ax = self._obj.plot(x='datetime', y=name, ax=ax, *args, **kwargs)
        return ax

# %% ../nbs/api/05_readers.ipynb 12
def load_json(filepath: str, # path to file
              metadata: Dict[str, str] = None, # metadata containing data_id, subject_id, or other_info
              ) -> Dict[str, pd.DataFrame]: # dictionary of wereable dataframes, one key:value pair per wereable data stream
    "Create a dataframe from a json containing a single or multiple streams of wereable data"
    # validate inputs
    if not isinstance(filepath, str):
        raise AttributeError("Filepath must be a string.")
    if metadata is not None:
        WereableAccessor._validate_metadata(metadata)
    # load json
    jdict = json.load(open(filepath, 'r'))
    # check that keys are valid
    for key in jdict.keys():
        if key not in VALID_WEREABLE_STREAMS:
            raise AttributeError("Invalid key in JSON file. Keys must be one of steps, heartrate, wake, light_estimate, or activity.")
    # create a df for each wereable stream
    df_dict = {key: pd.DataFrame.from_dict(jdict[key]) for key in jdict.keys()}
    for key in df_dict.keys():
        df = df_dict[key]
        # create datetime column
        if key == 'heartrate':
            df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')
        else:
            df['start'] = pd.to_datetime(df['start'], unit='s')
            df['end'] = pd.to_datetime(df['end'], unit='s')
            df['datetime'] = df['start']
        # add metadata to each df
        if metadata is not None:
            df.wereable.add_metadata(metadata, inplace=True)
        else:
            df.wereable.add_metadata({'data_id': 'unknown', 'subject_id': 'unknown'}, inplace=True)
        df_dict[key] = df
    return df_dict

# %% ../nbs/api/05_readers.ipynb 14
def load_csv(filepath: str, # full path to csv file to be loaded
             metadata: Dict[str, str] = None, # metadata containing data_id, subject_id, or other_info
             timestamp_col: str = None, # name of the column to be used as timestamp. If None, it is assumed that a `datetime` column exists
             *args, # arguments to pass to pd.read_csv
             **kwargs, # keyword arguments to pass to pd.read_csv
             ):
    "Create a dataframe from a csv containing wereable data"
    # validate inputs
    if not isinstance(filepath, str):
        raise AttributeError("Filepath must be a string.")
    if not isinstance(timestamp_col, str) and timestamp_col is not None:
        raise AttributeError("Timestamp column must be a string.")
    if metadata is not None:
        WereableAccessor._validate_metadata(metadata)
    # load csv
    df = pd.read_csv(filepath, *args, **kwargs)
    # create datetime column
    if timestamp_col is not None:
        df['datetime'] = pd.to_datetime(df[timestamp_col], unit='s')
    if timestamp_col is None and 'datetime' not in df.columns:
        raise AttributeError("CSV file must have a column named 'datetime' or a timestamp column must be provided.")
    # add metadata
    if metadata is not None:
        df.wereable.add_metadata(metadata, inplace=True)
    else:
        df.wereable.add_metadata({'data_id': 'unknown', 'subject_id': 'unknown'}, inplace=True)
    return df

# %% ../nbs/api/05_readers.ipynb 17
def load_actiwatch(filepath: str, # full path to csv file to be loaded
                   metadata: Dict[str, str] = None, # metadata containing data_id, subject_id, or other_info
                   *args, # arguments to pass to pd.read_csv
                   **kwargs, # keyword arguments to pass to pd.read_csv
                   ) -> pd.DataFrame: # dataframe with the wereable data
    "Create a dataframe from an actiwatch csv file"
    # validate inputs
    if not isinstance(filepath, str):
        raise AttributeError("Filepath must be a string.")
    if metadata is not None:
        WereableAccessor._validate_metadata(metadata)
    # load csv
    df = pd.read_csv(filepath, *args, **kwargs)
    df['datetime'] = pd.to_datetime(df['Date']+" "+df['Time'])
    # drop unnecessary columns
    df.drop(columns=['Date', 'Time'], inplace=True)
    # rename columns
    df.rename(columns=ACTIWATCH_COLUMN_RENAMING, inplace=True)
    # add metadata
    if metadata is not None:
        df.wereable.add_metadata(metadata, inplace=True)
    else:
        df.wereable.add_metadata({'data_id': 'unknown', 'subject_id': 'unknown'}, inplace=True)
    return df

# %% ../nbs/api/05_readers.ipynb 21
def resample_df(df: pd.DataFrame, # dataframe to be resampled
                freq: str = '1min', # frequency to resample to
                name: str = None, # name of the wereable data to resample (one of steps, heartrate, wake, light_estimate, or activity)
                ) -> pd.DataFrame: # resampled dataframe
    "Resample a wereable dataframe"
    # validate inputs
    if not df.wereable.is_valid():
        raise AttributeError("Dataframe must be a valid wereable dataframe.")
    if not isinstance(df, pd.DataFrame):
        raise AttributeError("Dataframe must be a pandas dataframe.")
    if not isinstance(freq, str):
        raise AttributeError("Frequency must be a string.")
    if name is not None and name not in VALID_WEREABLE_STREAMS:
        raise AttributeError(f"Name must be one of: {VALID_WEREABLE_STREAMS}.")
    if name not in df.columns:
        raise AttributeError(f"Name must be one of: {df.columns}.")
    # resample
    if 'start' in df.columns and 'end' in df.columns:
        intial_datetime = df.start.min()
        final_datetime = df.end.max()
        new_datetime = pd.date_range(intial_datetime, final_datetime, freq=freq)
        new_values = np.zeros(len(new_datetime))
        # ...
        new_df = pd.DataFrame({'datetime': new_datetime, name: new_values})
    else:
        intial_datetime = df.datetime.min()
        final_datetime = df.datetime.max()
        new_datetime = pd.date_range(intial_datetime, final_datetime, freq=freq)
        new_values = np.zeros(len(new_datetime))
        # ...
        new_df = pd.DataFrame({'datetime': new_datetime, name: new_values})
    return new_df

# %% ../nbs/api/05_readers.ipynb 25
def distribute_stream_over_duration(start: pd.Timestamp, # start of the stream
                                    end: pd.Timestamp, # end of the stream
                                    value: float, # value of the stream
                                    name: str, # name of the stream
                                    resample_freq: str, # frequency to resample the stream to
                                    ) -> pd.DataFrame: # dataframe with the distributed stream
    "Distribute a wereable datapoint over its duration"
    # validate inputs
    if not isinstance(start, pd.Timestamp):
        raise AttributeError("Start must be a pandas Timestamp.")
    if not isinstance(end, pd.Timestamp):
        raise AttributeError("End must be a pandas Timestamp.")
    if not isinstance(value, (int, float)):
        raise AttributeError("Value must be an integer or float.")
    if not isinstance(resample_freq, str):
        raise AttributeError("Resample frequency must be a string.")
    # create dataframe
    if pd.to_timedelta(resample_freq) > (end - start):
        datetime = pd.to_datetime(start + (end - start) / 2, unit='s')
        df = pd.DataFrame({'datetime': datetime, name: value}, index=[0])
    else:
        df = pd.DataFrame({'datetime': pd.date_range(start, end, freq=resample_freq)})
        df[name] = value / len(df)
    return df

# %% ../nbs/api/05_readers.ipynb 27
def combine_wereable_dataframes(df_dict: Dict[str, pd.DataFrame], # dictionary of wereable dataframes 
                                metadata: Dict[str, str] = None, # metadata containing for the combined dataframe resample_freq: str = '6T', # resampling frequency (e.g. '6T' for 6 minutes)
                                resample_freq: str = '6T', # resampling frequency (e.g. '6T' for 6 minutes)
                                ) -> pd.DataFrame: # combined wereable dataframe
    "Combine a dictionary of wereable dataframes into a single dataframe with resampling"
    df_list = []
    for name in df_dict.keys():
        df = df_dict[name]
        df.wereable.is_valid()
        if name in ['heartrate', 'light_estimate']:
            resampled_df = df.resample(
                                resample_freq, on='datetime'
                            ).agg(WEREABLE_RESAMPLE_METHOD[name])
            resampled_df.reset_index(inplace=True)
        else:
            # distribute wereable stream between start and end times
            distributed_df = pd.DataFrame(columns=['datetime', name])
            distribute_fn = lambda x: distribute_stream_over_duration(x.start, x.end, x[name], name, resample_freq)
            distributed_df = pd.concat(df.apply(distribute_fn, axis=1).tolist())
            distributed_df = distributed_df.groupby(distributed_df['datetime']).aggregate('sum') # if there are multiple values for the same datetime, add them
            distributed_df = distributed_df.reset_index(drop=False)
            # resample the distributed stream (to clean up those durations that are shorter than the resample frequency)
            resampled_df = distributed_df.resample(
                                resample_freq, on='datetime'
                            ).agg(WEREABLE_RESAMPLE_METHOD[name])
        if WEREABLE_NULL_VALUE_CONVERSION[name] is not None:
            resampled_df.replace(WEREABLE_NULL_VALUE_CONVERSION[name], inplace=True)
        df_list.append(resampled_df)
    # merge all dfs by datetime
    df = df_list[0]
    for i in range(1, len(df_list)):
        df = df.merge(df_list[i], on='datetime', how='outer')
    # sort by datetime
    df.sort_values(by='datetime', inplace=True)
    # add metadata
    if metadata is not None:
        df.wereable.add_metadata(metadata, inplace=True)
    else:
        df.wereable.add_metadata({'data_id': 'combined_dataframe'}, inplace=True)
    return df
